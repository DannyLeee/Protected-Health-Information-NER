{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "from data_loader import TalkDataset\n",
    "from model_budling import PHI_NER\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "type_classifier Linear(in_features=768, out_features=19, bias=True)\n",
      "BIO_classifier  Linear(in_features=768, out_features=3, bias=True)\n",
      "softmax         Softmax(dim=-1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "data_path = \"./dataset/sample_512_bert_data.pt\"\n",
    "\n",
    "list_of_dict = torch.load(data_path)\n",
    "# train_list = list_of_dict[:80]\n",
    "\n",
    "\"\"\" model setting (training)\"\"\"\n",
    "trainSet = TalkDataset(\"train\", list_of_dict)\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = PHI_NER()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5) # AdamW = BertAdam\n",
    "\n",
    "BIO_weight = torch.FloatTensor([89.94618834, 50.65151515,  1.]).cuda()\n",
    "type_weight = torch.FloatTensor([1, 7.42883207e+02, 3.78451358e+02, 4.71952716e+01,\n",
    " 9.70063365e+07, 9.70063365e+07, 6.68553928e+03, 9.70063365e+07,\n",
    " 9.70063365e+07, 9.70063365e+07, 9.70063365e+07, 9.70063365e+07,\n",
    " 9.70063365e+07, 2.00575855e+03, 9.70063365e+07, 5.49531139e+02,\n",
    " 9.70063365e+07, 3.10975750e+02, 9.70063365e+07]).cuda()\n",
    "\n",
    "BIO_loss_fct = nn.CrossEntropyLoss(weight=BIO_weight)\n",
    "type_loss_fct = nn.CrossEntropyLoss(weight=type_weight)\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "#             print(_)\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-17 22:08:26.817272+08:00\n",
      "2020-09-17 22:08:27.189362+08:00\t[epoch 1] loss: 4.048, type_loss: 5.871, BIO_loss: 1.112\n",
      "2020-09-17 22:08:27.532100+08:00\t[epoch 2] loss: 3.984, type_loss: 5.861, BIO_loss: 1.054\n",
      "2020-09-17 22:08:27.872703+08:00\t[epoch 3] loss: 3.929, type_loss: 5.848, BIO_loss: 1.005\n",
      "2020-09-17 22:08:28.214932+08:00\t[epoch 4] loss: 3.875, type_loss: 5.828, BIO_loss: 0.960\n",
      "2020-09-17 22:08:28.609836+08:00\t[epoch 5] loss: 3.841, type_loss: 5.816, BIO_loss: 0.933\n",
      "2020-09-17 22:08:28.949210+08:00\t[epoch 6] loss: 3.795, type_loss: 5.783, BIO_loss: 0.904\n",
      "2020-09-17 22:08:29.290107+08:00\t[epoch 7] loss: 3.749, type_loss: 5.749, BIO_loss: 0.875\n",
      "2020-09-17 22:08:29.629484+08:00\t[epoch 8] loss: 3.704, type_loss: 5.685, BIO_loss: 0.861\n",
      "2020-09-17 22:08:29.970222+08:00\t[epoch 9] loss: 3.659, type_loss: 5.634, BIO_loss: 0.842\n",
      "2020-09-17 22:08:30.312435+08:00\t[epoch 10] loss: 3.628, type_loss: 5.618, BIO_loss: 0.819\n"
     ]
    }
   ],
   "source": [
    "\"\"\" training \"\"\"\n",
    "from datetime import datetime,timezone,timedelta\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "EPOCHS = 10\n",
    "dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "print(dt2)\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    type_running_loss = 0.0\n",
    "    BIO_running_loss = 0.0\n",
    "    for data in trainLoader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, \\\n",
    "        type_label, BIO_label = [t.to(device) for t in data]\n",
    "\n",
    "    # 將參數梯度歸零\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(input_ids=tokens_tensors, \n",
    "                  token_type_ids=segments_tensors, \n",
    "                  attention_mask=masks_tensors)\n",
    "    \n",
    "    type_pred = outputs[0]\n",
    "    type_pred = torch.transpose(type_pred, 1, 2)\n",
    "#     print(type_pred.size(), type_label.size())\n",
    "    type_running_loss = type_loss_fct(type_pred, type_label)\n",
    "\n",
    "    BIO_pred = outputs[1]\n",
    "    BIO_pred = torch.transpose(BIO_pred, 1, 2)\n",
    "#     print(BIO_pred.size(), BIO_label.size())\n",
    "    BIO_loss = BIO_loss_fct(BIO_pred, BIO_label)\n",
    "\n",
    "    loss = BIO_loss + type_running_loss\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 紀錄當前 batch loss\n",
    "    running_loss += loss.item()\n",
    "    type_running_loss += type_running_loss.item()\n",
    "    BIO_running_loss += BIO_loss.item()\n",
    "\n",
    "    CHECKPOINT_NAME = './model/full_train_1_bert_wwm_E' + str(epoch) + '.pt' \n",
    "    torch.save(model.state_dict(), CHECKPOINT_NAME)\n",
    "\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "    print('%s\\t[epoch %d] loss: %.3f, type_loss: %.3f, BIO_loss: %.3f' %\n",
    "          (dt2, epoch + 1, running_loss, type_running_loss, BIO_running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path = \"./dataset/sample_512_bert_data.pt\"\n",
    "t = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(t)):\n",
    "    print(i, len(t[i]['input_ids']), len(t[i]['seg']), len(t[i]['att']), len(t[i]['BIO_label']), len(t[i]['type_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "print(tokenizer.convert_ids_to_tokens(t[63]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
